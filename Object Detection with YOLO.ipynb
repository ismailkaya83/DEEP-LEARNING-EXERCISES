{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR WEBCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0) # If there is a single camera connected, just pass 0.\n",
    "\n",
    "\n",
    "classNames = []\n",
    "with open('coco.names', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(\"yolov3-320.cfg\", \"yolov3-320.weights\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "\n",
    "def findObjects(outputs,img):\n",
    "    \n",
    "    hT, wT, cT = img.shape\n",
    "    bbox = [] # bounding box, w, h, x, y\n",
    "    classIds = [] # this list contains all the class ids\n",
    "    confs = [] # this list contains confidence values\n",
    "\n",
    "    for output in outputs: # there are three outputs\n",
    "        for det in output: # we check every detection in outputs\n",
    "            scores = det[5:] # we need the probability scores so we are checking after 5th index\n",
    "            classId = np.argmax(scores) # we find index of maximum probability scores\n",
    "            confidence = scores[classId] # we find number of probability\n",
    "            if confidence > 0.5: # we select bigger than 0.5\n",
    "                w,h = int(det[2]*wT) , int(det[3]*hT) # width and height of box\n",
    "                x,y = int((det[0]*wT) - w/2) , int((det[1]*hT) - h/2) # center of box\n",
    "                bbox.append([x, y, w, h])\n",
    "                classIds.append(classId)\n",
    "                confs.append(float(confidence))\n",
    "            \n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(bbox, confs, 0.6, 0.5) \n",
    "    \n",
    "    \"\"\"\n",
    "        bboxes          : a set of bounding boxes to apply NMS.\n",
    "        scores          : a set of corresponding confidences.\n",
    "        score_threshold : a threshold used to filter boxes by score.\n",
    "        nms_threshold   : a threshold used in non maximum suppression.\n",
    "        indices         : the kept indices of bboxes after NMS.\n",
    "        eta             : a coefficient in adaptive threshold formula: nms_thresholdi+1=eta⋅nms_thresholdi.\n",
    "        top_k           :if >0, keep at most top_k picked indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = bbox[i]\n",
    "        x,y,w,h = box[0],box[1],box[2],box[3]\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,255),2)\n",
    "        cv2.putText(img,f'{classNames[classIds[i]].upper()} {int(confs[i]*100)}%',(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.6,\n",
    "                    (255,0,255),2)\n",
    "    \n",
    "while True:\n",
    "    \n",
    "    success, img = cap.read()\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255, (320,320), [0,0,0], 1, crop=False)\n",
    "    \n",
    "    \"\"\"\n",
    "        image        : input image (with 1-, 3- or 4-channels).\n",
    "        size         : spatial size for output image\n",
    "        mean         : scalar with mean values which are subtracted from channels. \n",
    "                       Values are intended to be in (mean-R, mean-G, mean-B) order if image has BGR ordering and \n",
    "                       swapRB is true.\n",
    "        scalefactor  : multiplier for image values.\n",
    "        swapRB       : flag which indicates that swap first and last channels in 3-channel image is necessary.\n",
    "        crop         : flag which indicates whether image will be cropped after resize or not\n",
    "        ddepth       : Depth of output blob. Choose CV_32F or CV_8U.\n",
    "        \n",
    "        if crop is true, input image is resized so one side after resize is equal to corresponding dimension in size \n",
    "        and another one is equal or larger. Then, crop from the center is performed. \n",
    "        If crop is false, direct resize without cropping and preserving aspect ratio is performed.\n",
    "    \"\"\"\n",
    "    \n",
    "    net.setInput(blob) # we transfer videos to network this is input value in network\n",
    "    \n",
    "    \n",
    "    layerNames = net.getLayerNames() # we select layer names in network\n",
    "    \n",
    "    last_layer_index = net.getUnconnectedOutLayers() # we take index number of output layer end of the network\n",
    "    \n",
    "    outputNames = [layerNames[i[0] -1] for i in last_layer_index]\n",
    "    \n",
    "    outputs = net.forward(outputNames)\n",
    "\n",
    "    findObjects(outputs, img)\n",
    "    \n",
    "    cv2.imshow('Image', img)\n",
    "    \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR VIDEOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(\"video/pedestrians.mp4\")\n",
    "\n",
    "\n",
    "classNames = []\n",
    "with open('coco.names', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(\"yolov3-320.cfg\", \"yolov3-320.weights\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "\n",
    "def findObjects(outputs,img):\n",
    "    \n",
    "    hT, wT, cT = img.shape\n",
    "    bbox = [] # bounding box, w, h, x, y\n",
    "    classIds = [] # this list contains all the class ids\n",
    "    confs = [] # this list contains confidence values\n",
    "\n",
    "    for output in outputs: # there are three outputs\n",
    "        for det in output: # we check every detection in outputs\n",
    "            scores = det[5:] # we need the probability scores so we are checking after 5th index\n",
    "            classId = np.argmax(scores) # we find index of maximum probability scores\n",
    "            confidence = scores[classId] # we find number of probability\n",
    "            if confidence > 0.5: # we select bigger than 0.5\n",
    "                w,h = int(det[2]*wT) , int(det[3]*hT) # width and height of box\n",
    "                x,y = int((det[0]*wT) - w/2) , int((det[1]*hT) - h/2) # center of box\n",
    "                bbox.append([x, y, w, h])\n",
    "                classIds.append(classId)\n",
    "                confs.append(float(confidence))\n",
    "            \n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(bbox, confs, 0.6, 0.5) \n",
    "    \n",
    "    \"\"\"\n",
    "        bboxes          : a set of bounding boxes to apply NMS.\n",
    "        scores          : a set of corresponding confidences.\n",
    "        score_threshold : a threshold used to filter boxes by score.\n",
    "        nms_threshold   : a threshold used in non maximum suppression.\n",
    "        indices         : the kept indices of bboxes after NMS.\n",
    "        eta             : a coefficient in adaptive threshold formula: nms_thresholdi+1=eta⋅nms_thresholdi.\n",
    "        top_k           :if >0, keep at most top_k picked indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = bbox[i]\n",
    "        x,y,w,h = box[0],box[1],box[2],box[3]\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,255),2)\n",
    "        cv2.putText(img,f'{classNames[classIds[i]].upper()} {int(confs[i]*100)}%',(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.6,\n",
    "                    (255,0,255),2)\n",
    "    \n",
    "while True:\n",
    "    \n",
    "    success, img = cap.read()\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255, (320,320), [0,0,0], 1, crop=False)\n",
    "    \n",
    "    \"\"\"\n",
    "        image        : input image (with 1-, 3- or 4-channels).\n",
    "        size         : spatial size for output image\n",
    "        mean         : scalar with mean values which are subtracted from channels. \n",
    "                       Values are intended to be in (mean-R, mean-G, mean-B) order if image has BGR ordering and \n",
    "                       swapRB is true.\n",
    "        scalefactor  : multiplier for image values.\n",
    "        swapRB       : flag which indicates that swap first and last channels in 3-channel image is necessary.\n",
    "        crop         : flag which indicates whether image will be cropped after resize or not\n",
    "        ddepth       : Depth of output blob. Choose CV_32F or CV_8U.\n",
    "        \n",
    "        if crop is true, input image is resized so one side after resize is equal to corresponding dimension in size \n",
    "        and another one is equal or larger. Then, crop from the center is performed. \n",
    "        If crop is false, direct resize without cropping and preserving aspect ratio is performed.\n",
    "    \"\"\"\n",
    "    \n",
    "    net.setInput(blob) # we transfer videos to network this is input value in network\n",
    "    \n",
    "    \n",
    "    layerNames = net.getLayerNames() # we select layer names in network\n",
    "    \n",
    "    last_layer_index = net.getUnconnectedOutLayers() # we take index number of output layer end of the network\n",
    "    \n",
    "    outputNames = [layerNames[i[0] -1] for i in last_layer_index]\n",
    "    \n",
    "    outputs = net.forward(outputNames)\n",
    "\n",
    "    findObjects(outputs, img)\n",
    "    \n",
    "    cv2.imshow('Image', img)\n",
    "    \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"images/busy_street.jpg\")\n",
    "#img = cv2.resize(img, None, fx=0.3, fy=0.3)\n",
    "\n",
    "\n",
    "classNames = []\n",
    "with open('coco.names', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(\"yolov3-320.cfg\", \"yolov3-320.weights\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "\n",
    "def findObjects(outputs,img):\n",
    "    \n",
    "    hT, wT, cT = img.shape\n",
    "    bbox = [] # bounding box, w, h, x, y\n",
    "    classIds = [] # this list contains all the class ids\n",
    "    confs = [] # this list contains confidence values\n",
    "\n",
    "    for output in outputs: # there are three outputs\n",
    "        for det in output: # we check every detection in outputs\n",
    "            scores = det[5:] # we need the probability scores so we are checking after 5th index\n",
    "            classId = np.argmax(scores) # we find index of maximum probability scores\n",
    "            confidence = scores[classId] # we find number of probability\n",
    "            if confidence > 0.5: # we select bigger than 0.5\n",
    "                w,h = int(det[2]*wT) , int(det[3]*hT) # width and height of box\n",
    "                x,y = int((det[0]*wT) - w/2) , int((det[1]*hT) - h/2) # center of box\n",
    "                bbox.append([x, y, w, h])\n",
    "                classIds.append(classId)\n",
    "                confs.append(float(confidence))\n",
    "            \n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(bbox, confs, 0.6, 0.5) \n",
    "    \n",
    "    \"\"\"\n",
    "        bboxes          : a set of bounding boxes to apply NMS.\n",
    "        scores          : a set of corresponding confidences.\n",
    "        score_threshold : a threshold used to filter boxes by score.\n",
    "        nms_threshold   : a threshold used in non maximum suppression.\n",
    "        indices         : the kept indices of bboxes after NMS.\n",
    "        eta             : a coefficient in adaptive threshold formula: nms_thresholdi+1=eta⋅nms_thresholdi.\n",
    "        top_k           :if >0, keep at most top_k picked indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = bbox[i]\n",
    "        x,y,w,h = box[0],box[1],box[2],box[3]\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,255),2)\n",
    "        cv2.putText(img,f'{classNames[classIds[i]].upper()} {int(confs[i]*100)}%',(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.6,\n",
    "                    (255,0,255),2)\n",
    "    \n",
    "while True:\n",
    "    \n",
    " \n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255, (320,320), [0,0,0], 1, crop=False)\n",
    "    \n",
    "    \"\"\"\n",
    "        image        : input image (with 1-, 3- or 4-channels).\n",
    "        size         : spatial size for output image\n",
    "        mean         : scalar with mean values which are subtracted from channels. \n",
    "                       Values are intended to be in (mean-R, mean-G, mean-B) order if image has BGR ordering and \n",
    "                       swapRB is true.\n",
    "        scalefactor  : multiplier for image values.\n",
    "        swapRB       : flag which indicates that swap first and last channels in 3-channel image is necessary.\n",
    "        crop         : flag which indicates whether image will be cropped after resize or not\n",
    "        ddepth       : Depth of output blob. Choose CV_32F or CV_8U.\n",
    "        \n",
    "        if crop is true, input image is resized so one side after resize is equal to corresponding dimension in size \n",
    "        and another one is equal or larger. Then, crop from the center is performed. \n",
    "        If crop is false, direct resize without cropping and preserving aspect ratio is performed.\n",
    "    \"\"\"\n",
    "    \n",
    "    net.setInput(blob) # we transfer videos to network this is input value in network\n",
    "    \n",
    "    \n",
    "    layerNames = net.getLayerNames() # we select layer names in network\n",
    "    \n",
    "    last_layer_index = net.getUnconnectedOutLayers() # we take index number of output layer end of the network\n",
    "    \n",
    "    outputNames = [layerNames[i[0] -1] for i in last_layer_index]\n",
    "    \n",
    "    outputs = net.forward(outputNames)\n",
    "\n",
    "    findObjects(outputs, img)\n",
    "    \n",
    "    cv2.imshow('Image', img)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
